# 特征工程

Q1：**特征工程与特征选择之间有什么区别？**

- 特征工程允许我们从已有的特征中创建新的特征，以帮助机器学习模型做出更有效和准确的预测。一些典型的特征工程的任务包括：
    - 填充变量中的缺失值。
    - 将分类变量编码为数字。
    - 变量转换。
    - 从数据集中可用的特征中创建或提取新特征。
- 特征选择允许我们从特征池（包括任何新设计的特征）中选择特征，这将有助于机器学习模型更有效地对目标变量进行预测。特征选择方法需要监督学习，并根据所训练模型在验证数据集上的性能进行评估。
- 在典型的机器学习流水线中，在完成特征工程后进行特征选择

Q2：**无监督学习可以帮助解决哪些常见的机器学习问题？**

- 数据稀缺：对于监督学习，需要大量的标签数据才能使模型表现良好。无监督学习可以通过聚类等方法自动标记未标记的示例。
- 异常值：数据的质量非常重要，数据中的异常值将会严重影响机器学习算法的性能。无监督学习可以使用降维来执行异常值检测，并剔除异常值，为正常数据创建解决方案。
- 特征工程：特征工程是劳动密集型的任务，需要人类创造性地设计特征。无监督学习方法，如representation learning，可自动学习正确类型的特征以构建特征工程。

Q3：***说出特征选择*的一些好处**

- 高维的特征和低样本数/特征数比会在数据集中引入噪声，算法会更容易过拟合
- 减少特征数量会减少程序的运行时间。同时，较少的特征量允许你能够使用更复杂的算法，搜索更多超参数或进行更多次评估。
- 较小的特征集更容易被人类理解。这将使你能够专注于可预测性的主要来源，并进行更精确的特征工程。

Q4：**解释One-Hot encoding和label encoding。应用这些技术后数据集的维度是增加还是减少？**

- label encoding为每个*唯一的类别值*分配了一个*整数值*。例如，在“*颜色*”变量中，“*红色*”是`0`，“*绿色*”是`1`，“*蓝色*”是`2`等。这些整数值彼此之间具有自然的有序关系，机器学习算法能够理解和利用这种关系。这种编码方式不会影响数据集的维度。

![Untitled](Untitled%2067.png)

- One-Hot Encoding将每个唯一的整数值转换为一个新的向量，向量中只含有0和1，整数值决定向量的位置，因此数据集的维度增加了。

![Untitled](Untitled%2068.png)

**Q5：讲述机器学习算法应该包括哪些主要流程？**

- 机器学习处理流水线应包含训练机器学习模型所涉及的一系列步骤，涉及预处理、特征选择、分类/回归和后处理。例如，分类模型的基本管道包含：
    - Scaler：用于预处理数据，将数据转换为零均值和单位方差
    - 特征选择器，丢弃相关性强的特征，减少冗余
    - 模型

Q6：**可以使用PCA做特征选择吗？**

- PCA不是一种特征选择技术；特征选择是指从完整的特征集中选择特征的子集，PCA获得的内容是Principal Components，是所有原始特征变量集的线性组合

Q7：**如何用F-test方法来选择特征？**

- F-test是一种统计检验方法，用于比较模型并检查模型之间的差异是否显着
- F-Test 执行假设检验模型 X 和 Y，其中 X 是由常量创建的模型，Y 是由常量和特征创建的模型。比较两个模型中的最小二乘误差，并检查模型 X 和 Y 之间的误差差异是否显著

引用：[https://towardsdatascience.com](https://towardsdatascience.com/why-how-and-when-to-apply-feature-selection-e9c69adfabf2)

Q8：**你知道哪些特征选择方法？**

- 特征选择方法一般分为无监督方法与有监督方法
- ****无监督特征选择方法：****
    - 丢弃接近零方差的特征（零方差意味着信息量少）、丢弃具有许多缺失值的特征、丢弃高度强相关的特征
- ****有监督特征选择方法：****
    - Wrappers：使用模型对不同的特征子集进行评分，最终选择最佳特征。每个新子集都用于训练一个模型，然后在保留集上评估其性能。选择产生最佳模型性能的特征子集
    - Fliters：为了评估每个特征的有用性，简单地分析特征与模型目标的统计关系，使用相关性或互信息等度量作为模型性能指标的代理
    - Embedded：
        - LASSO 回归方法，本质上只是一种正则化方法，使得特征权重在模型训练中缩小到零。结果，许多特征最终的权重为零，这意味着它们从模型中被丢弃；
        - AE Bottleneck方法，使用神经网络做特征选择；

![Untitled](Untitled%2069.png)

引用：[https://towardsdatascience.com/](https://towardsdatascience.com/)

Q9：**如何提高随机森林的性能？**

- 通过改变 **nodesize 和 tree size**，如果想增加预测能力，选择更小的节点大小和更多的树。树会更大并一直走到尽头，树的数量也会增加，因此它们的整体表现会更好

Q10：**如何对未知特征（不明白特征的真实意义）进行特征工程？**

- 当没有太多领域知识却拥有大量特征时，手动特征工程相对没有意义，此时最适合进行自动化的特征工程
- 自动化的特征工程包括：
    - 主成分分析 (PCA) 和独立成分分析 (ICA) ，将现有数据映射到另一个特征空间
    - • 深度特征合成 ，允许从神经网络的中间层传输中间学习

引用：[https://datascience.stackexchange.com/questions/10640/how-to-perform-feature-engineering-on-unknown-features](https://datascience.stackexchange.com/questions/10640/how-to-perform-feature-engineering-on-unknown-features)

Q11：**讲述PCA的具体执行流程？**

- 1.**特征标准化**。我们将每个特征标准化，使其均值为 0，方差为 1；具有不同数量级值的特征会阻止 PCA 计算最佳主成分，因为PCA对变量的方差非常敏感；
- 2.**计算协方差矩阵**；*协方差矩阵是一个$d$*维度的方阵，它显示了每个特征之间的成对特征相关性；
- 3.**计算协方差矩阵的特征分解**。我们计算协方差矩阵的特征向量（单位向量）及其相关的特征值（与特征向量相乘的标量）；
- 4.**从最高特征值到最低特征值对特征向量进行排序**。具有最高特征值的特征向量是第一主成分。更高的特征值对应于解释的更大量的共享方差；
- 5.**选择主成分的数量**。选择前 N 个特征向量（基于它们的特征值）成为 N 个主成分；
- 6.****沿主成分轴重铸数据：****使用由协方差矩阵的特征向量形成的特征向量，将数据从原始轴重新定向到由主成分表示的轴，这可以通过将原始数据集的转置乘以特征向量的转置来完成。

Q12：**解释PCA（主成分分析）和ICA（独立成分分析）之间的区别？**

- 尽管这两种方法可能看起来相关，但它们执行不同的任务。具体来说，PCA常用于压缩信息即降维，而 ICA 旨在通过将输入空间转换为最大独立基来分离信息；ICA更侧重于独立性，即独立成分

Q13：**如何将偏态分布转变为正态分布？**

- 机器学习问题中，如果预测变量和目标变量呈正态分布，则模型更倾向于做出可靠的预测
- 将偏移的数据分布转变为正态分布的方法通常有：
    - ****对数变换，在所有数据上施加log()变换****
    - **平方根变换**

引用：[https://towardsdatascience.com/top-3-methods-for-handling-skewed-data-1334e0debf45](https://towardsdatascience.com/top-3-methods-for-handling-skewed-data-1334e0debf45)

Q14：**你知道哪些数据的插值方法？**

- 对于连续数据：
    - 如果数据呈正态分布，则使用平均值。
    - 如果数据倾斜或有很多异常值，使用中值。
- 对于类别数据：
    - 如果数据是可排序的，则使用中值。
    - 如果数据不可排序，使用模式。
- 对于布尔特征：
    - 使用频率插值

![Untitled](Untitled%2070.png)

Q15：**多元线性回归中，应该去除高度相关的特征吗？**

- 如果两个特征高度相关，建议我们在数据集中只保留一个特征；这是因为线性回归的最小二乘解：$w=(X^{T}X)^{-1}X^{T}y$，假设因变量$y$服从方差为$\delta$的正态分布，则上述权重向量的方差为$\delta(X^{T}X)^{-1}$，为了使模型足够稳定，上述方差应该很低。
- 由矩阵的奇异值分解可知，当拥有高度相关的特征时，$w$的方差会很大，影响模型的稳定性

引用：[https://datascience.stackexchange.com/questions/36404/when-to-remove-correlated-variables](https://datascience.stackexchange.com/questions/36404/when-to-remove-correlated-variables)

Q16：**如何在迁移学习中运用特征工程？**

- 如果一个模型是基于足够大且通用的数据集训练的，那么该模型将有效地充当一系列任务的通用模型。随后，您可以利用这个大模型学习的特征映射，完成feature engineering；
- 只需在预训练模型上添加一个将从头开始训练的新分类器，这样便可重复利用先前针对数据集学习的特征映射

、

Q17：**标准化和归一化异同**

- 俩种方案都是线性变换，将原有数据进行了放缩，归一化一般放缩到[0,1]，标准化则转为服从标准正态分布，数据的大小顺序没有发生改变；影响归一化的主要是两个极值，而标准化里每个数据都会影响，因为计算了均值和标准差。

Q18：**如何理解组合特征？**

- 组合特征其实就是特征的交叉。举个例子，比方说浏览新闻，给用户推荐新闻的时候如果按性别或者兴趣这个特征来推，可能效果不是特别好，但是经过观察发现，性别和兴趣有比较强的关系，例如女性可能更喜欢看娱乐类，男性更喜欢体育财经类，把这俩特征做组合加入到模型，效果比单特征要强很多。

Q19：**如何处理高维组合特征？比如用户ID和内容ID？**

- 使用矩阵分解，常用的矩阵分解有
    - QR分解：Q是正交矩阵，R是上三角矩阵
    - LU分解
    - SVD
    - Jordan分解：分成多个Jordan块，每个Jordan块都是对角线是相同的特征值，次对角线是1。
    

Q20：**文本特征表示有哪些模型？他们的优缺点都是什么？**

- 词袋模型：词汇需要仔细的设计，特别是为了管理文档的大小，这会影响文档表示的稀疏性；简单但是失去了词的上下文结构；由于计算的原因（空间和时间复杂性）以及信息的原因，稀疏表示更难模拟，因为模型在如此庞大的代表空间中利用这么少的信息面临着巨大挑战
- N-gram：将连续出现的n个词组成词组作为一个单独的特征放到向量表示中，缺点是参数空间过大；数据稀疏
- 主题模型：能够计算出每篇文章的主题分布
- 词嵌入模型：将词映射成K维的向量；能表示词与词之间的含义关系；

Q21：**Word2vec和LDA两个模型有什么区别和联系？**

- LDA利用文档中单词的共现关系来对单词按主题聚类，也可以理解为对“文档-单词”矩阵进行分解，得到“文档-主题”和“主题-单词”两个概率分布，而 Word2Vec其实是对“上下文-单词”矩阵进行学习，其中上下文由周围的几个单词组成，由此得到的刺向了表示更多地融入了上下文共现的特征，也就是说，如果两个单词所对应的Word2Vec向量相似度高，那么他们很可能经常在相同的上下文中出现。
- 主题模型和词嵌入两类方法最大的不同其实在于模型本身，主题模型是一种基于概率图模型的生成式模型，其似然函数可以写成若干条件概率连乘的形式，其中包括需要推测的隐含变量（即主题），而词嵌入模型一般表达为神经网络的形式，似然函数定义在网络的输出之上，需要通过学习网络的权重得到单词的稠密向量表示。

Q22：**怎么检测特征中的异常值？**

- 基于高斯分布的异常值检测：3σ原则也是属于高斯分布判断方法的一种，在这里异常值被定义为，其值与平均值的偏差超过三倍标准差的值，在正态分布的假设下，区域u+3σ包含了99.7% 的数据，如果某个值距离分布的均值超过了3σ，那么这个值就可以被简单的标记为一个异常点：P(|x−μ|>3σ)≤0.003
- 四分位数
- 更多的还有基于各类统计量来检测多元离群点，例如x^2检验、t检验等。
- 基于主成分分析的矩阵分解方法，这种方法经过主成分分析分解，再进行重构，通过异常值在主成分分量上 的偏差更大来判断是否异常。
- 基于距离，利用聚类的思想，对数据进行聚类.，排除距离中心最远的N个点，一般的方法有，kmeans、knn、DBSCAN等。

Q23：**怎么处理特征中的异常值？**

- 删除含有异常值的记录，将异常值视为缺失值，交给缺失值处理方法来处理；
- 或者用平均值或者众数来修正。

引用：[https://blog.csdn.net/](https://blog.csdn.net/)

Q24：**如何解决数据不平衡的问题？**

- 采样的方法，采样分为上采样（Oversampling）和下采样（Undersampling），上采样是把小众类复制多份，下采样是从大众类中剔除一些样本，或者说只从大众类中选取部分样本。上采样后的数据集中会重复出现一些样本，训练出来的模型会有一定的过拟合，解决办法，可以在每次生成新数据点时加入轻微的随机扰动；而下采样的缺点也比较明显，那就是最终的训练集丢失了数据，模型只学到了一部分。可能会导致欠拟合，解决办法：多次下采样（放回采样，这样产生的训练集才相互独立）产生多个不同的训练集，进而训练多个不同的分类器，利用模型融合，通过组合多个分类器的结果得到最终的结果。
- 数据合成方法，利用小众样本在特征空间的相似性来生成新样本，这类方法在小数据场景下有很多成功案例，比如医学图像分析等。
- 对于数据极端不平衡时，可以观察观察不同算法在同一份数据下的训练结果的precision和recall，这样做有两个好处，一是可以了解不同算法对于数据的敏感程度，二是可以明确采取哪种评价指标更合适。针对机器学习中的数据不平衡问题，建议更多PR(Precision-Recall曲线)，而非ROC曲线，具体原因画图即可得知，如果采用ROC曲线来作为评价指标，很容易因为AUC值高而忽略实际对少量样本的效果其实并不理想的情况。
- 选择对数据倾斜相对不敏感的算法，如树模型。

引用：[https://blog.csdn.net/](https://blog.csdn.net/)

Q25：**简述线性判别分析LDA的原理**

- LDA是一种监督学习的降维技术，也就是说它的数据集的每个样本是有类别输出的。这点和PCA不同，PCA是不考虑样本类别输出的无监督降维技术。LDA的思想可以用一句话来概括，就是“投影后类内方差最小，类间方差最大”。我们要将数据在低维度上进行投影，投影后希望每一种类别数据的投影点尽可能地接近，而不同类别的数据的类别中心之间的距离尽可能地大。